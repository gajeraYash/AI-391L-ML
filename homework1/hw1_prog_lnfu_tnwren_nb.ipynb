{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9kIqgkMgEXab"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import tree, ensemble\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import KFold\n",
    "from collections import defaultdict\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LehKy8iWh7tt"
   },
   "outputs": [],
   "source": [
    "# Assignment Constants\n",
    "RANDOM_STATE = 10\n",
    "FIGSIZE = (12,8)\n",
    "#### Use the following line before plt.plot(....) to increase the plot size ####\n",
    "# plt.figure(figsize=FIGSIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sOzKw8qBh7tt"
   },
   "source": [
    "## Question 1 \n",
    "Use the breast cancer data set from Homework 0 to create a training set. Recall that the\n",
    "label is 0 if the patient’s data indicates a malignant cancer and 1 otherwise. Compute the\n",
    "base rate of malignant cancer occurrence over the entire data set. In other words, what would\n",
    "be your best guess for the probability of malignant cancer of a single example using only the\n",
    "labels in the training set? This question is very simple, so try not to overthink it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4XhEGZi_h7tv",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cancer = load_breast_cancer()\n",
    "p_malignant = np.mean(cancer.target)\n",
    "base_rate = min(p_malignant, 1 - p_malignant)\n",
    "print(f\"Base Rate: {base_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mKWeUAVHh7tv"
   },
   "source": [
    "## 2\n",
    "The goal is to build a decision tree that, based on the other features in the set, predicts\n",
    "whether or not a patient has malignant cancer. So this is a classification problem. Using\n",
    "`tree.DecisionTreeClassifier` and other functions in the scikit-learn library, one can build\n",
    "a decision tree and calculate both its training accuracy when fitted to the entire data set as\n",
    "well as its accuracy using 10-fold cross validation (which gives a better idea of true accuracy).\n",
    "In this question you will need to complete two sub-components:\n",
    "### (a) \n",
    "(a) Make a plot visualizing the performance of a `tree.DecisionTreeClassifier` as you\n",
    "search for an optimal `max_depth` parameter. Vary the depth of your decision tree using\n",
    "max depth = 1,2,. . . ,10 and record the results from the following evaluation procedures\n",
    "for each setting:\n",
    "* The accuracy when training and testing on the full dataset.\n",
    "* 10-fold cross-validated accuracy.\n",
    "\n",
    "Plot the results of both evaluation procedures on the same plot with evaluation scores on\n",
    "the y-axis and max depth values on the x-axis. Use 10 as your random seed/state for the\n",
    "decision tree and the cross-validation. Use a legend to label both evaluation procedures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6CBo0UxNh7tv"
   },
   "outputs": [],
   "source": [
    "# Part 2:\n",
    "\n",
    "x, y = cancer.data, cancer.target # Features and Labels\n",
    "max_depths = range(1, 11) # Max depths to try for Decision Trees\n",
    "\n",
    "# accuracy storage\n",
    "training_accuracy = []\n",
    "cross_validation_accuracy = []\n",
    "\n",
    "# K-Fold Cross Validation\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "for depth in max_depths:\n",
    "    # Decision Tree Classifier\n",
    "    decision_tree_clf = tree.DecisionTreeClassifier(max_depth=depth, random_state=RANDOM_STATE)\n",
    "    \n",
    "    # Train on full dataset and record training accuracy\n",
    "    decision_tree_clf.fit(x, y)\n",
    "    training_accuracy.append(decision_tree_clf.score(x, y)) # Training accuracy\n",
    "\n",
    "    # perform K-Fold Cross Validation (10 folds)\n",
    "    cv_scores = cross_val_score(decision_tree_clf, x, y, cv=kf)\n",
    "    cross_validation_accuracy.append(np.mean(cv_scores))\n",
    "\n",
    "# Plot Results\n",
    "plt.figure(figsize=FIGSIZE)\n",
    "plt.plot(max_depths, training_accuracy, label=\"Training Accuracy\")\n",
    "plt.plot(max_depths, cross_validation_accuracy, label=\"Cross-Validation Accuracy (10-fold)\")\n",
    "plt.xlabel(\"Max Depth\")\n",
    "plt.xticks(max_depths)\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend(fontsize='large')\n",
    "plt.grid(True, linewidth=0.3, alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2b and 2c: Best accuracies and depths\n",
    "\n",
    "print(\"\\nDepth\\tTraining Acc.\\tCV Acc.\")\n",
    "for depth, train_acc, cv_acc in zip(max_depths, training_accuracy, cross_validation_accuracy):\n",
    "    print(f\"{depth}\\t{train_acc:.4f}\\t\\t{cv_acc:.4f}\")  \n",
    "\n",
    "best_full_accuracy = max(training_accuracy)\n",
    "best_cv_accuracy = max(cross_validation_accuracy)\n",
    "best_full_depth = max_depths[training_accuracy.index(best_full_accuracy)]\n",
    "best_cv_depth = max_depths[cross_validation_accuracy.index(best_cv_accuracy)]\n",
    "\n",
    "print(f\"\\nBest Full Set Training Accuracy: {best_full_accuracy:.4f} at depth {best_full_depth}\"\n",
    "      f\"\\nBest Cross-Validation Accuracy: {best_cv_accuracy:.4f} at depth {best_cv_depth}\"\n",
    "      f\"\\nBase Rate: {base_rate:.4f}\")\n",
    "\n",
    "best_full_set_depths = [depth for depth, acc in zip(max_depths, training_accuracy) if acc == best_full_accuracy]\n",
    "best_cv_set_depths = [depth for depth, acc in zip(max_depths, cross_validation_accuracy) if acc == best_cv_accuracy]\n",
    "print(f\"Depths with Best Full Set Training Accuracy: {best_full_set_depths}\")\n",
    "print(f\"Depths with Best Cross-Validation Accuracy: {best_cv_set_depths}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sx9GOh0th7tv"
   },
   "source": [
    "### (b) \n",
    "Answer the questions below based on the results of 2a. Write your answers in the\n",
    "corresponding field in the markdown cell that is present in the HW1 template notebook.\n",
    "Do this by double clicking the markdown cell and writing your answer directly in the\n",
    "cell. Pressing enter will re-render the markdown.\n",
    "\n",
    "#### (i.)\n",
    "What setting of `max_depth` gave the best accuracy w.r.t. the **full-dataset** accuracy?\n",
    "If more than one setting equaled the best accuracy, list each of the best settings.\n",
    "\n",
    "**Student answer here:** [7,8,9,10]\n",
    "\n",
    "#### (ii.)\n",
    "What setting of `max_depth`  gave the best accuracy w.r.t. the **cross-\n",
    "validated** accuracy? If more than one setting equaled the best accuracy, list each of the best settings.\n",
    "\n",
    "**Student answer here:** [5]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0vKsAO_-h7tw"
   },
   "source": [
    "## 3\\. \n",
    "This question explores random forest classifiers by using scikit-learn’s `ensemble.RandomForestClassifier`.\n",
    "You will make two plots and answer questions about them.\n",
    "\n",
    "### (a) \n",
    "For the first plot, use a `ensemble.RandomForestClassifier` and the best depth you\n",
    "found 2(b)ii as `max_depth`. We will now find the optimal setting of a second parameter,\n",
    "n estimators. Vary the number of trees in the forest via the parameter `n_estimators`\n",
    "and plot its 10-fold cross-validated accuracy (use `n_estimators` = 1, 2, . . . , 20). Again,\n",
    "use 10 as your random seed for your classifier and cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tDwWUsgEh7tw"
   },
   "outputs": [],
   "source": [
    "# Part 3:\n",
    "\n",
    "# we will use the variable best_cv_depth from above as max_depth\n",
    "n_estimators_range = range(1, 21)\n",
    "cv_accuracy_rf = []\n",
    "\n",
    "for n_estimators in n_estimators_range:\n",
    "    rf_clf = ensemble.RandomForestClassifier(n_estimators=n_estimators, max_depth=best_cv_depth, random_state=RANDOM_STATE)\n",
    "\n",
    "    # Calculate 10-fold cross-validated accuracy\n",
    "    cv_scores_rf = cross_val_score(rf_clf, x, y, cv=kf)\n",
    "    cv_accuracy_rf.append(np.mean(cv_scores_rf))\n",
    "\n",
    "# Plot Random Forest Results\n",
    "plt.figure(figsize=FIGSIZE)\n",
    "plt.plot(n_estimators_range, cv_accuracy_rf, label=\"Random Forest CV Accuracy (10-fold)\")\n",
    "plt.xlabel(\"Number of Estimators\")\n",
    "plt.xticks(n_estimators_range)\n",
    "plt.ylabel(\"CV Accuracy\")\n",
    "plt.title(\"max_depth = {}\".format(best_cv_depth))\n",
    "plt.legend(fontsize='large')\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rf_accuracy = max(cv_accuracy_rf)\n",
    "best_rf_n_estimator = n_estimators_range[cv_accuracy_rf.index(best_rf_accuracy)]\n",
    "best_rf_full_set_n_estimators = [n_estimators for n_estimators, acc in zip(n_estimators_range, cv_accuracy_rf) if acc == best_rf_accuracy]\n",
    "print(f\"\\nBest Random Forest CV Accuracy: {best_rf_accuracy:.4f} at n_estimators {best_rf_n_estimator}\")\n",
    "print(f\"n_estimators with Best Random Forest CV Accuracy: {best_rf_full_set_n_estimators}\")\n",
    "\n",
    "print(\"\\nN_Estimators\\tCV Acc.\")\n",
    "for n_estimators, cv_acc in zip(n_estimators_range, cv_accuracy_rf):\n",
    "    print(f\"{n_estimators}\\t\\t{cv_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vJWRlyfNh7tw"
   },
   "source": [
    "### (b) \n",
    "Do you see an improvement using random forests versus using a single tree? (Note: use\n",
    "the `n_estimators`=1 result as the result for a single tree.)\n",
    "\n",
    "**Student answer here:** Yes, there is an improvement in accruacy of ~0.03 when comparing random forest to single tree.\n",
    "\n",
    "### (c) \n",
    "What setting of `n_estimators` gave the best accuracy w.r.t. the cross-validated ac-\n",
    "curacy?\n",
    "\n",
    "**Student answer here:** 16\n",
    "\n",
    "### (d) \n",
    "For the second plot, again use a `ensemble.RandomForestClassifier`, but this time\n",
    "you will fix the `n_estimators` parameter and again attempt to find the optimal setting\n",
    "of a `max_depth`. Use your answer to 3c as the setting for `n_estimators` and follow the\n",
    "procedure from 2a to find the best setting for max depth. This time, only plot the results\n",
    "from cross-validation and not the full set, but the plot should be the same structure\n",
    "as in 2a otherwise (use `max_depth` = 1,2,. . . ,10). Again, use 10 as your random seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D-eI83pxh7tw"
   },
   "outputs": [],
   "source": [
    "# Part 3d:\n",
    "\n",
    "# we will use the best_rf_n_estimator variable from above as n_estimators\n",
    "cv_accuracy_rf = []\n",
    "for depth in max_depths:\n",
    "    rf_clf = ensemble.RandomForestClassifier(n_estimators=best_rf_n_estimator, max_depth=depth, random_state=RANDOM_STATE)\n",
    "\n",
    "    # Calculate 10-fold cross-validated accuracy\n",
    "    cv_scores_rf = cross_val_score(rf_clf, x, y, cv=kf)\n",
    "    cv_accuracy_rf.append(np.mean(cv_scores_rf))\n",
    "\n",
    "# Plot Results\n",
    "plt.figure(figsize=FIGSIZE)\n",
    "plt.plot(max_depths, cv_accuracy_rf, label=\"Random Forest CV Accuracy (10-fold)\")\n",
    "plt.xlabel(\"Max Depth\")\n",
    "plt.xticks(max_depths)\n",
    "plt.ylabel(\"CV Accuracy\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rf_accuracy = max(cv_accuracy_rf)\n",
    "best_rf_n_estimator = n_estimators_range[cv_accuracy_rf.index(best_rf_accuracy)]\n",
    "best_rf_full_set_n_estimators = [n_estimators for n_estimators, acc in zip(n_estimators_range, cv_accuracy_rf) if acc == best_rf_accuracy]\n",
    "print(f\"\\nBest Random Forest CV Accuracy: {best_rf_accuracy:.4f} at n_estimators {best_rf_n_estimator}\")\n",
    "print(f\"n_estimators with Best Random Forest CV Accuracy: {best_rf_full_set_n_estimators}\")\n",
    "\n",
    "print(\"\\nN_Estimators\\tCV Acc.\")\n",
    "for n_estimators, cv_acc in zip(n_estimators_range, cv_accuracy_rf):\n",
    "    print(f\"{n_estimators}\\t\\t{cv_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FZCOIqxph7tx"
   },
   "source": [
    "### (e) \n",
    "In the plot in 3d, is the optimal setting of `max_depth` the same as in 2(b)ii? If not, what\n",
    "is the new optimal setting of `max_depth`?\n",
    "\n",
    "**Student answer here:** No, The best setting now is [6].\n",
    "\n",
    "## 4\\. \n",
    "For this last question, we will explore the dependability of our estimates. \n",
    "### (a) \n",
    "Make a plot using the following procedure:\n",
    "#### i. \n",
    "Using random state values from 0, 1, · · · , 99 calculate the 10-fold cross-validation\n",
    "accuracy of different `tree.DecisionTreeClassifiers` with max depth settings from\n",
    "1, 2, · · · , 10.\n",
    "As before, you should use the same random state value for your classifier and cross-validation.\n",
    "#### ii. \n",
    "Then record the best max depth settings for each random state. Be sure to check whether multiple settings\n",
    "achieve the best accuracy.\n",
    "\n",
    "\n",
    "Plot the counts for the best max depth settings as a bar chart with the max depth settings on the x-axis and the 'best parameter counts' on the y-axis (number of times that parameter was selected as the best max depth setting).\n",
    "\n",
    "*Note*: this calculation might take some time. For debugging, try a smaller range of\n",
    "random states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D-Ira43Bh7tx"
   },
   "outputs": [],
   "source": [
    "# Part 4: \n",
    "\n",
    "random_states = range(100)\n",
    "# we will use max_depths variable from above\n",
    "best_max_depth_results = defaultdict(list)\n",
    "\n",
    "for i, rs in enumerate(random_states):\n",
    "    # kfold with current random state\n",
    "    kf_rs = KFold(n_splits=10, shuffle=True, random_state=rs)\n",
    "\n",
    "    # store cross validation means for each depth\n",
    "    cv_means = []\n",
    "    for depth in max_depths:\n",
    "        decision_tree_clf = tree.DecisionTreeClassifier(max_depth=depth, random_state=rs)\n",
    "        cv_scores = cross_val_score(decision_tree_clf, x, y, cv=kf_rs)\n",
    "        cv_means.append(np.mean(cv_scores))\n",
    "\n",
    "    # find the best accuracy and corresponding depths for this random state\n",
    "    best_acc = max(cv_means)\n",
    "    best_depth = [depth for depth, acc in zip(max_depths, cv_means) if acc == best_acc]\n",
    "\n",
    "    # record each winning depth for this random state\n",
    "    for depth in best_depth:\n",
    "        best_max_depth_results[depth].append(best_acc)\n",
    "    print(f\"{i+1}/{len(random_states)} states completed\\r\", end=\"\")\n",
    "\n",
    "# convert\n",
    "depths = list(best_max_depth_results.keys())\n",
    "counts = [len(best_max_depth_results[d]) for d in depths]\n",
    "\n",
    "plt.figure(figsize=FIGSIZE)\n",
    "plt.bar(depths, counts)\n",
    "plt.xlabel('Max Depth')\n",
    "plt.ylabel('Counts')\n",
    "plt.xticks(depths)\n",
    "plt.grid(axis='y', linewidth=0.3, alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nDepth\\tCount\\tAvg Acc.\")\n",
    "for depth in sorted(best_max_depth_results.keys()):\n",
    "    count = len(best_max_depth_results[depth])\n",
    "    avg_acc = np.mean(best_max_depth_results[depth])\n",
    "    print(f\"{depth}\\t{count}\\t{avg_acc:.4f}\")\n",
    "\n",
    "top_depths = sorted(best_max_depth_results.keys(), key=lambda d: len(best_max_depth_results[d]), reverse=True)[:2]\n",
    "print(\"\\nTop 2 Depths by Count:\")\n",
    "for depth in top_depths:\n",
    "    count = len(best_max_depth_results[depth])\n",
    "    avg_acc = np.mean(best_max_depth_results[depth])\n",
    "    print(f\"Depth: {depth}, Count: {count}, Avg Acc: {avg_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UJvUvKu7h7tx"
   },
   "source": [
    "### (b) \n",
    "What are the top two most frequent parameter settings?\n",
    "\n",
    "**Student answer here:** \n",
    "\n",
    "Depth: 5, Count: 36, Avg Acc: 0.9357\n",
    "\n",
    "Depth: 4, Count: 30, Avg Acc: 0.9333"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of hw1p_template.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
